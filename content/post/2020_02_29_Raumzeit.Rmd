---
title: "Analyzing a 10 Year old Podcast"
author: "Nils Ratnaweera"
date: "2020-02-29T22:00:00+01:00"
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, echo = FALSE)
```

I've been listening to the german Postcat ["Raumzeit"](http://raumzeit-podcast.de) for quite a while now, and I had the idea to visualize some metrics of the podcast using R. I was interested to know what data I can aquire from the podcast and what I could do within it. 

I could've aquired quite amount of data on the podcast using it's RSS feed together with the library `tidyRSS`, but not all episodes were in that feed so I decided to scrape the podcast's website instead. 


```{r, warning=FALSE,message=FALSE}
library(tidyverse)
library(lubridate)
library(rvest)
library(ggrepel)
library(scales)

theme_set(theme_classic())
```


```{r, cache = TRUE}
show <- read_html("https://raumzeit-podcast.de/archiv/") %>%
  html_nodes(".show")


titles <- show %>%
  html_nodes(".show__title") %>%
  html_text() %>%
  str_trim("both")

dates <- show %>%
  html_nodes(".show__meta-data.show__meta-data--date") %>%
  html_text() %>%
  str_trim("both") %>%
  as.POSIXct(format = "%d.%m.%Y")

dur <- show %>%
  html_nodes(".show__meta-data.show__meta-data--duration") %>%
  html_text() %>%
  str_trim("both") %>%
  str_remove_all("\t|\n")


hours <- as.integer(str_match(dur,"(\\d) Stunde")[,2])
minutes <- as.integer(str_match(dur,"(\\d+) Minute")[,2])
durations <- map2_dbl(hours,minutes,~sum(.x*60,.y,na.rm = TRUE))


item_link <- show %>% 
  html_nodes(".show__title__link") %>%
  html_attr("href")


folgenuebersicht <- tibble(title = titles, date = dates,duration = durations, item_link = item_link) %>%
  mutate(
    rz_folge = as.integer(str_match(title,"RZ(\\d{3})\\s")[,2]),
    title_clean = str_remove(title,"RZ\\d{3}\\s")
  )
```




## Number of Episodes per Year

Während in den Anfangsjahren zwischen 4 und 7 Podcasts produziert wurden, erfuhr die Sendung im dritten Jahr einen kleinen Rückgang und eine Pause im vierten Jahr (2015) wo nur zwei Sendungen veröffentlicht wurden. Auch 2017 viel mit drei Podcasts eher Mager aus, wir hoffen auf eine Zunahme in den kommenden Jahren.  

```{r}

folgenuebersicht %>%
  group_by(year = year(date)) %>%
  count() %>%
  ggplot(aes(year,n)) + 
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = 2010:2020) +
  labs(x = "Year",y = "Number of Episodes")
```


```{r}

folgenuebersicht_filter_high <- filter(folgenuebersicht,duration > 150)
folgenuebersicht_filter_low <- filter(folgenuebersicht,duration < 50)

folgenuebersicht %>%
  ggplot(aes(date, duration,label = title_clean)) +
  geom_point() +
  geom_point(data = folgenuebersicht_filter_high, colour = "red") +
  geom_point(data = folgenuebersicht_filter_low, colour = "red") +
  geom_label_repel(
    data = folgenuebersicht_filter_high,
    min.segment.length = 0,
    nudge_y = 170-folgenuebersicht_filter_high$duration,
    force = 10) +
  geom_label_repel(data = folgenuebersicht_filter_low,min.segment.length = 0) +
  expand_limits(y = c(0,200))+
  labs(x = "Release Date",y = "Episode Duration")
```


```{r, cache = TRUE}
replies <- map_dfr(folgenuebersicht$item_link,function(x){
  vcard <- x %>%
    read_html() %>%
    html_nodes(".comment-author.vcard") %>%
    html_text() %>%
    str_remove_all("\t|\n") 

  who <- str_match(vcard,"(.+)\\ssagte")[,2]
  datum <- str_match(vcard,"\\sam\\s(\\d+\\.\\s\\w+\\s\\d{4})\\sum")[,2]
  zeit <- str_match(vcard,"\\sum\\s(\\d+:\\d+)\\s")[,2]
  
  datumzeit <- as.character(as.POSIXct(paste(datum,zeit),format = "%d. %B %Y %H:%M"))
  tibble(item_link = x, reply_author = who, reply_date = datumzeit)
})
```


```{r}
# https://stackoverflow.com/a/43626186/4139249
replies <- replies %>%
  mutate(reply_date = as.POSIXct(reply_date))

folgen_replies <- left_join(folgenuebersicht,replies,by = "item_link")

c_trans <- function(a, b, breaks = b$breaks, format = b$format) {
  a <- as.trans(a)
  b <- as.trans(b)

  name <- paste(a$name, b$name, sep = "-")

  trans <- function(x) a$trans(b$trans(x))
  inv <- function(x) b$inverse(a$inverse(x))

  trans_new(name, trans, inverse = inv, breaks = breaks, format=format)

}

rev_date <- c_trans("reverse", "time")
ggplot(folgen_replies, aes(date,reply_date,colour = factor(date))) +
  geom_line()+
  geom_point() +
  scale_y_continuous(trans = rev_date)+
  geom_point(data = folgen_replies, aes(date,date), colour = "black") +
  theme(
    legend.position = "none",
    # axis.text.x = element_blank(),
    # axis.ticks.x = element_blank(),
    # axis.title.x = element_blank()
    ) +
  labs(y = "Episode Release Date\nand Date of Reply", x = "Episode Release Date") +
  coord_equal()
```


```{r}
folgen_replies_filter <-  filter(folgen_replies,year(date)<2015)

ggplot(folgen_replies_filter,aes(date,reply_date,colour = factor(date))) +
  geom_line()+
  geom_point() +
  scale_y_continuous(trans = rev_date)+
  geom_point(data = folgen_replies_filter, aes(date,date), colour = "black") +
  theme(
    legend.position = "none",
    # axis.text.x = element_blank(),
    # axis.ticks.x = element_blank(),
    # axis.title.x = element_blank()
    ) +
  labs(y = "Episode Release Date\nand Date of Reply", x = "Episode Release Date") +
  coord_equal()
```


```{r}
folgen_replies_smry <- folgen_replies %>%
  group_by_at(1:6) %>%
  count(name = "Replies") %>%
  arrange(-Replies) %>%
  ungroup() %>%
  mutate(
    title_clean = fct_reorder(title_clean,Replies)
    )

folgen_replies_smry %>%
  head(20) %>%
  ggplot(aes(title_clean,Replies)) +
  geom_col() +
  ggplot2::coord_flip() +
  labs(y = "# Replies",x = "")

folgen_replies_smry %>%
  ggplot(aes(date,Replies)) +
  geom_point() +
  geom_smooth(method = "loess",se = FALSE) +
  labs(y = "# Replies",x = "Release Date")
```


This is very nice, but I'm not sure if that is all we can get out of the listners reaction to the podcast. I know Tim Pritlove is also an active twitterer, so let's see what we can get from his tweets. Do do this, I use the package `rtweet` for which I have created an app as descriebed [here](https://rtweet.info/articles/auth.html).

```{r, eval = FALSE}
library(rtweet)

token <- create_token(
  app = "my_appname",
  consumer_key = "my_key",
  consumer_secret = "my_secret_key")
```

I can now get the tweets by \@raumzeit using the `get_timeline` function. 

```{r, eval = FALSE}
raumtweets <- get_timeline("raumzeit", n = 3200)
```

```{r, include=FALSE}
load("raumtweets.Rda")
```

I perticularly interested in the tweets regarding a specific podcast episode. Looking through the tweets I see that Tim usually uses the prefix "RZ" to refer to an episode, regretably, he is not completely concise in this. Non the less, let's see how many 

```{r}
raumtweets$rz_folge <- as.integer(str_match(raumtweets$text,"RZ(\\d{1,3})")[,2])

raumtweets %>%
  group_by(rz_folge) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(10)

sum(!is.na(raumtweets$rz_folge))
```

We've got 133 assigned tweets and 445 NAs with up to 5 tweets for a single episode. However, there is another way to assign a tweet to an episode: Via the URL linking to the epiode. So let's extract these URLs from the column `urls_expanded_url`.

```{r}
raumtweets$item_link <- map_chr(raumtweets$urls_expanded_url,function(x){
  out <- x[str_detect(x,"raumzeit-podcast.de/\\d{4}")]
  ifelse(length(out)<0,NA_character_,out)
  })

```



```{r}

raumtweets_folgen <- raumtweets %>%
  left_join(select(filter(folgenuebersicht,!is.na(rz_folge)),title_clean,date,item_link), by = "item_link") %>%
  left_join(select(filter(folgenuebersicht,!is.na(rz_folge)),title_clean,date,rz_folge), by = "rz_folge") %>%
  mutate(
    title_clean = coalesce(title_clean.x,title_clean.y),
    date = coalesce(date.x,date.y)
  ) %>%
  select(-c(title_clean.x,title_clean.y,date.x,date.y)) %>%
  filter(!is.na(title_clean))

raumtweets_folgen_smry <- raumtweets_folgen %>%
  group_by(title_clean,date) %>% 
  summarise_at(vars(favorite_count,retweet_count,reply_count),max, na.rm = TRUE)


raumtweets_folgen_smry_filter <- filter(raumtweets_folgen_smry,retweet_count>30)
ggplot(raumtweets_folgen_smry, aes(date,retweet_count,label = title_clean)) +
  geom_point() +
  geom_point(data = raumtweets_folgen_smry_filter,colour = "red") +
  geom_smooth(method = "loess",se = FALSE) +
  geom_label_repel(data = raumtweets_folgen_smry_filter,nudge_x = 5,nudge_y = 5) +
  labs(x = "Publish Date", y = "Retweet Count (Maxium)")

raumtweets_folgen_smry_filter = filter(raumtweets_folgen_smry,favorite_count>70)
ggplot(raumtweets_folgen_smry, aes(date,favorite_count,label = title_clean)) +
  geom_point() +
  geom_point(data = raumtweets_folgen_smry_filter, colour = "red") +
  geom_smooth(method = "loess",se = FALSE)  +
  geom_label_repel(data = raumtweets_folgen_smry_filter, nudge_x = 5,nudge_y = 5) +
  labs(x = "Publish Date", y = "Favorites Count (Maxium)")

```


