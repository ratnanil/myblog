---
title: "Analysing a 10 Year old Podcast"
author: "Nils Ratnaweera"
date: "2020-02-29T22:00:00+01:00"
categories: ["R"]
tags: ["webscraping","podcast","R"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$get()
if(is.null(knitr::opts_knit$get("rmarkdown.pandoc.to"))){
  setwd("content/post/")
}
```

I've been listening to the German astronomy podcast ["Raumzeit"](http://raumzeit-podcast.de) for quite a while now, and I had the idea to visualize some metrics of the podcast using R. I was interested to know what data I can acquire from the podcast and what I could do within it. 

I started out using the podcast's RSS feed with the library `tidyRSS`, but realized that not all episodes were _in_ that feed. I therefore decided to scrape the podcast's website instead. I also used `rtweet` to gather some metrics about the podcast using the tweets by the podcast's host Tim Pritlove, for which he uses the handle \@raumzeit. I won't show all code in this post, but you can get the whole Rmd file on my github page.


I use the following packages for this analysis, and set the ggplot default theme to `theme_classic`:

```{r, warning=FALSE,message=FALSE}
library(tidyverse)
library(lubridate)
library(rvest)
library(ggrepel)
library(scales)
library(rtweet)

theme_set(theme_classic())
```


## Scraping the site

Using chrome's element inspector, I retrieved the css class names of the objects that I wanted to extract from the site, as in the following example.

```{r, eval = FALSE}
class_show <- read_html("https://raumzeit-podcast.de/archiv/") %>%
  html_nodes(".show")

titles <- class_show %>%
  html_nodes(".show__title") %>%
  html_text() %>%
  str_trim("both")
```


```{r, eval = FALSE, include=FALSE}
dates <- class_show %>%
  html_nodes(".show__meta-data.show__meta-data--date") %>%
  html_text() %>%
  str_trim("both") %>%
  as.POSIXct(format = "%d.%m.%Y")

dur <- class_show %>%
  html_nodes(".show__meta-data.show__meta-data--duration") %>%
  html_text() %>%
  str_trim("both") %>%
  str_remove_all("\t|\n")


hours <- as.integer(str_match(dur,"(\\d) Stunde")[,2])
minutes <- as.integer(str_match(dur,"(\\d+) Minute")[,2])
durations <- map2_dbl(hours,minutes,~sum(.x*60,.y,na.rm = TRUE))


item_link <- class_show %>% 
  html_nodes(".show__title__link") %>%
  html_attr("href")
```

Once I had all the data together, I gathered them in a `tibble` and started my analysis.

```{r, eval = FALSE, include=FALSE}
folgenuebersicht <- tibble(title = titles, date = dates,duration = durations, item_link = item_link) %>%
  mutate(
    rz_folge = as.integer(str_match(title,"RZ(\\d{3})\\s")[,2]),
    title_clean = str_remove(title,"RZ\\d{3}\\s")
  )

save(folgenuebersicht,file = "folgenuebersicht.Rda")
```


```{r, include = FALSE}

load("folgenuebersicht.Rda")
```



```{r, cache = TRUE}
head(folgenuebersicht)
```


### Number of Episodes per Year

The podcast has had different phases over the years and the number of podcast's released each year varies significantly. The first Episode was released end of 2010. In 2011, Tim Pritlove published 25 episodes, the highest number of episodes he would ever publish thereafter. His project was initially funded externally and when the sponsors ended their contact in 2013, Tim took a break to regroup and find a different method to finance his podcast. He found a way and as of 2015, has be been steadily growing the number of episodes published each year.

```{r, echo = FALSE}

folgenuebersicht %>%
  group_by(year = year(date)) %>%
  count() %>%
  ggplot(aes(year,n)) + 
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = 2010:2020) +
  labs(x = "Year",y = "Number of Episodes")
```


### Episode duration

What is remarkable about this podcast, is that Tim takes as much time as he needs with each guest, and doesn't really keep an eye on the time. In his `r nrow(folgenuebersicht)` podcasts to date he has spent over `r floor(sum(folgenuebersicht$duration)/60)` hours speaking to his guests, which means an avarage of `r floor(sum(folgenuebersicht$duration))/nrow(folgenuebersicht)` minutes per episode.

This means that the podcast duration can very significantly, and I had wondered if there was some trend over time. Apparently, this does not seem to be the case, as shown in the following graph:

```{r, echo = FALSE}

folgenuebersicht_filter_high <- filter(folgenuebersicht,duration > 150)
folgenuebersicht_filter_low <- filter(folgenuebersicht,duration < 50)

folgenuebersicht %>%
  ggplot(aes(date, duration,label = title_clean)) +
  geom_point() +
  geom_point(data = folgenuebersicht_filter_high, colour = "red") +
  geom_point(data = folgenuebersicht_filter_low, colour = "red") +
  geom_label_repel(
    data = folgenuebersicht_filter_high,
    min.segment.length = 0,
    nudge_y = 170-folgenuebersicht_filter_high$duration,
    force = 10) +
  geom_label_repel(data = folgenuebersicht_filter_low,min.segment.length = 0) +
  expand_limits(y = c(0,200))+
  labs(x = "Release Date",y = "Episode Duration (in Minutes)")
```

The two very short episodes are small housekeeping / information podcasts of which Tim has only produced two.

## Listener interaction


I was interested to know how intense the interaction was in regard to each podcast. I found two metrics with which this can be quantified: 

- The number of replies posted on the [podcat's website](https://raumzeit-podcast.de/)
- Twitter metrics (retweets, favourites, replies) of tweets Tim Pritlove posted after the appearance of a podcast with the user handle \@raumzeit.


### Website replies

I scraped the website to retrieve the replies of year podcast using the podcast's link an again, `rvest`:

```{r}
head(folgenuebersicht)
```


```{r, eval = FALSE}
replies <- map_dfr(folgenuebersicht$item_link,function(x){
  comment <- x %>%
    read_html() %>%
    html_nodes(".comment") %>%
    html_text() %>%
    str_remove_all("\t|\n") 
  vcard <- map_chr(comment,~str_split_fixed(.x,"\\s:",2)[1])
  comment_text <- map_chr(comment,~str_split_fixed(.x,"\\s:",2)[2])
  
  who <- str_match(vcard,"(.+)\\ssagte")[,2]
  datum <- str_match(vcard,"\\sam\\s(\\d+\\.\\s\\w+\\s\\d{4})\\sum")[,2]
  zeit <- str_match(vcard,"\\sum\\s(\\d+:\\d+)")[,2]

  datumzeit <- as.character(as.POSIXct(paste(datum,zeit),format = "%d. %B %Y %H:%M"))
  tibble(item_link = x, reply_author = who, reply_date = datumzeit, comment = comment_text)
}) %>%
  mutate(reply_date = as.POSIXct(reply_date))
```

```{r, eval = FALSE, include = FALSE}
save(replies, file = "replies.Rda")
```

```{r, include = FALSE}
load("replies.Rda")
```


```{r}
head(replies)
```

### Website replies

I was expecting some podcasts to have a high listener interaction and much discussion over time while others probabbly provoked little or no listener interaction. Old episodes have had a longer time period for such interaction, and so i wanted to visualize the intensity of the interaction (i.e. replies) in such a way, that this aspect becomes apparent.

```{r, echo = FALSE}
# https://stackoverflow.com/a/43626186/4139249


folgen_replies <- left_join(folgenuebersicht,replies,by = "item_link")

c_trans <- function(a, b, breaks = b$breaks, format = b$format) {
  a <- as.trans(a)
  b <- as.trans(b)

  name <- paste(a$name, b$name, sep = "-")

  trans <- function(x) a$trans(b$trans(x))
  inv <- function(x) b$inverse(a$inverse(x))

  trans_new(name, trans, inverse = inv, breaks = breaks, format=format)

}

rev_date <- c_trans("reverse", "time")
```


```{r, echo = FALSE}

ggplot(folgen_replies, aes(rz_folge,reply_date,colour = factor(rz_folge),label = title_clean)) +
  geom_line()+
  geom_point(colour = "white", size = 3) +
  geom_point() +
  scale_colour_viridis_d() +
  scale_y_continuous(trans = rev_date)+
  theme(legend.position = "none",
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(y = "Date of Reply", x = "Episode Number")
```


```{r}
folgen_replies_smry <- folgen_replies %>%
  group_by_at(1:6) %>%
  count(name = "Replies") %>%
  arrange(-Replies) %>%
  ungroup() %>%
  mutate(
    title_clean = fct_reorder(title_clean,Replies)
    )
```


```{r}
folgen_replies_smry %>%
  head(20) %>%
  ggplot(aes(title_clean,Replies)) +
  geom_col() +
  ggplot2::coord_flip() +
  labs(y = "# Replies",x = "")
```


```{r}

folgen_replies_smry_filter <- filter(folgen_replies_smry, Replies > 40)

folgen_replies_smry %>%
  ggplot(aes(date,Replies, label = title_clean)) +
  geom_point() +
  geom_point(data = folgen_replies_smry_filter, colour = "red") +
  geom_label_repel(data = folgen_replies_smry_filter,min.segment.length = 0) +
  geom_smooth(method = "loess",se = FALSE) +
  labs(y = "# Replies",x = "Release Date")
```


```{r}
library(tidytext)
folgen_replies_unnest <- folgen_replies %>%
  unnest_tokens(word,comment)

stopwords <- tidytext::get_stopwords("de")

folgen_replies_unnest <- folgen_replies_unnest %>%
  anti_join(stopwords, by = "word")


folgen_replies_unnest %>%
  count(word,sort = TRUE) 

```


```{r}

sent <- c(
  # positive Wörter
  readLines("SentiWS_v2.0/SentiWS_v2.0_Positive.txt",
            encoding = "UTF-8"),
  # negative Wörter
  readLines("SentiWS_v2.0/SentiWS_v2.0_Negative.txt",
            encoding = "UTF-8")
) %>% lapply(function(x) {
  # Extrahieren der einzelnen Spalten
  res <- strsplit(x, "\t", fixed = TRUE)[[1]]
  return(data.frame(words = res[1], value = res[2],
                    stringsAsFactors = FALSE))
}) %>%
  bind_rows %>% 
  mutate(words = gsub("\\|.*", "", words) %>% tolower,
         value = as.numeric(value)) %>% 
  # manche Wörter kommen doppelt vor, hier nehmen wir den mittleren Wert
  group_by(words) %>% summarise(value = mean(value)) %>% ungroup



folgen_replies_unnest <- inner_join(folgen_replies_unnest,sent,by = c("word" = "words"))

folgen_reply_sent <- folgen_replies_unnest %>%
  group_by_at(1:7) %>%
  summarise(value = mean(value)) %>%
  group_by_at(1:6) %>%
  summarise(
    median = median(value),
    q25 = quantile(value,0.25),
    q75 = quantile(value,0.75),
    n = n(),
  ) 
ggplot(folgen_reply_sent, aes(n,median,ymin = q25,ymax = q75, colour = factor(rz_folge))) +
  # geom_point() +,
  geom_pointrange() +
  geom_hline(yintercept = 0) +
  labs(x = "Number of replies",y = "Mean sentiment") +
  lims(y = c(-1,1)) +
  theme(legend.position = "none")

folgen_reply_sent_fil <- folgen_reply_sent %>%
  filter(median > 0.3 | median< -0.1)

folgen_reply_sent %>%
  ggplot(aes(factor(rz_folge),median,label = title_clean)) +
  geom_point() +
  # geom_label_repel(data = folgen_reply_sent_fil,nudge_y = folgen_reply_sent_fil$value) +
  scale_y_continuous(limits = c(-1,1)) +
  labs(x = "Raumzeit-Folge",y = "Mittleres 'sentiment'")


folgen_replies_unnest %>%
  group_by(word,value) %>%
  count() %>%
  arrange(desc(value))

folgen_replies_unnest %>%
  filter(title_clean == "Mars Express") %>%
  pull(word)

```


### Twitter Data

```{r, eval = FALSE, warning=FALSE}
token <- create_token(
  app = "my_appname",
  consumer_key = "my_key",
  consumer_secret = "my_secret_key")
```

I can now get the tweets by \@raumzeit using the `get_timeline` function. 

```{r, eval = FALSE}
raumtweets <- get_timeline("raumzeit", n = 3200)
```

```{r, include=FALSE}
load("raumtweets.Rda")
raumtweets <- raumtweets %>%
  filter(!is_retweet) %>%
  dplyr::select(created_at,text,favorite_count,retweet_count,urls_expanded_url)
```

```{r}
head(raumtweets)
```

I particularly interested in the tweets regarding a specific podcast episode. Looking through the tweets I see that Tim usually uses the prefix "RZ" to refer to an episode, regrettably, he is not completely concise in this. Non the less, let's see how many episodes we can matchi in this way.

```{r, echo = TRUE}
raumtweets$rz_folge <- as.integer(str_match(raumtweets$text,"RZ(\\d{1,3})")[,2])
```

We've got 133 assigned tweets and 445 NAs with up to 5 tweets for a single episode. However, there is another way to assign a tweet to an episode: Via the URL linking to the episode. So let's extract these URLs from the column `urls_expanded_url`.

```{r, echo = TRUE}
raumtweets$item_link <- map_chr(raumtweets$urls_expanded_url,function(x){
  out <- x[str_detect(x,"raumzeit-podcast.de/\\d{4}")]
  ifelse(length(out)<0,NA_character_,out)
  })

```

We can see how many tweets we were able to assign with the two methods using `table()`
This shows that the first method was more successful than the second method.

```{r, echo = TRUE}
table(!is.na(raumtweets$rz_folge),!is.na(raumtweets$item_link))
```


```{r}
raumtweets_folgen <- raumtweets %>%
  left_join(select(filter(folgenuebersicht,!is.na(rz_folge)),title_clean,date,item_link), by = "item_link") %>%
  left_join(select(filter(folgenuebersicht,!is.na(rz_folge)),title_clean,date,rz_folge), by = "rz_folge") %>%
  mutate(
    title_clean = coalesce(title_clean.x,title_clean.y),
    date = coalesce(date.x,date.y)
  ) %>%
  select(-c(title_clean.x,title_clean.y,date.x,date.y)) %>%
  filter(!is.na(title_clean))


raumtweets_folgen_filter <- filter(raumtweets_folgen,retweet_count>30)
```


```{r}
ggplot(raumtweets_folgen, aes(created_at,retweet_count,label = title_clean)) +
  geom_point() +
  theme(legend.position = "none") +
  geom_point(data = raumtweets_folgen_filter,colour = "red") +
  geom_smooth(method = "loess",se = FALSE) +
  geom_label_repel(data = raumtweets_folgen_filter,nudge_x = 5,nudge_y = 5) +
  labs(x = "Tweet Date", y = "Retweet Count")

raumtweets_folgen_filter <- filter(raumtweets_folgen,favorite_count>70)
ggplot(raumtweets_folgen, aes(created_at,favorite_count,label = title_clean)) +
  geom_point() +
  geom_point(data = raumtweets_folgen_filter, colour = "red") +
  geom_smooth(method = "loess",se = FALSE)  +
  geom_label_repel(data = raumtweets_folgen_filter, nudge_x = 5,nudge_y = 5) +
  labs(x = "Tweet Date", y = "Favourites Count")

```


