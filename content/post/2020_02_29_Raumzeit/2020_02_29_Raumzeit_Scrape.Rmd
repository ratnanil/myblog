---
title: "Scraping a podcast's Metadata"
author: "Nils Ratnaweera"
date: "2020-02-29T22:00:00+01:00"
categories: ["R"]
tags: ["webscraping","podcast","R"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, eval = FALSE)
knitr::opts_chunk$get()
if(is.null(knitr::opts_knit$get("rmarkdown.pandoc.to"))){
  setwd("content/post/2020_02_29_Raumzeit")
}
```

I've been listening to the German astronomy podcast ["Raumzeit"](http://raumzeit-podcast.de) for quite a while now, and I had the idea to visualize some metrics of the podcast using R. I was interested to know what data I can acquire from the podcast and what I could do within it. 

I started out using the podcast's RSS feed with the library `tidyRSS`, but realized that not all episodes were _in_ that feed. I therefore decided to scrape the podcast's website instead. I also used `rtweet` to gather some metrics about the podcast using the tweets by the podcast's host Tim Pritlove, for which he uses the handle \@raumzeit. I won't show all code in this post, but you can get the whole Rmd file on my github page.


I use the following packages for this analysis, and set the ggplot default theme to `theme_classic`:

```{r, warning=FALSE,message=FALSE}
library(tidyverse)
library(lubridate)
library(rvest)
library(rtweet)
```


## Scraping the site

Using chrome's element inspector, I retrieved the css class names of the objects that I wanted to extract from the site, as in the following example.

```{r}
class_show <- read_html("https://raumzeit-podcast.de/archiv/") %>%
  html_nodes(".show")

titles <- class_show %>%
  html_nodes(".show__title") %>%
  html_text() %>%
  str_trim("both")
```


```{r}
dates <- class_show %>%
  html_nodes(".show__meta-data.show__meta-data--date") %>%
  html_text() %>%
  str_trim("both") %>%
  as.POSIXct(format = "%d.%m.%Y")

dur <- class_show %>%
  html_nodes(".show__meta-data.show__meta-data--duration") %>%
  html_text() %>%
  str_trim("both") %>%
  str_remove_all("\t|\n")


hours <- as.integer(str_match(dur,"(\\d) Stunde")[,2])
minutes <- as.integer(str_match(dur,"(\\d+) Minute")[,2])
durations <- map2_dbl(hours,minutes,~sum(.x*60,.y,na.rm = TRUE))


item_link <- class_show %>% 
  html_nodes(".show__title__link") %>%
  html_attr("href")
```

Once I had all the data together, I gathered them in a `tibble` and started my analysis.

```{r}
folgenuebersicht <- tibble(title = titles, date = dates,duration = durations, item_link = item_link) %>%
  mutate(
    rz_folge = as.integer(str_match(title,"RZ(\\d{3})\\s")[,2]),
    title_clean = str_remove(title,"RZ\\d{3}\\s")
  )

write_csv(folgenuebersicht, "folgenuebersicht.csv")
```




## Listener interaction


I was interested to know how intense the interaction was in regard to each podcast. I found two metrics with which this can be quantified: 

- The replies posted on the [podcat's website](https://raumzeit-podcast.de/)
- Twitter metrics (retweets, favourites, replies) of tweets Tim Pritlove posted after the appearance of a podcast with the user handle \@raumzeit.


### Website replies

Again, using `rvest` I scraped the website to retrieve the replies for each podcast. 


```{r}
replies <- map_dfr(folgenuebersicht$item_link,function(x){
  comment <- x %>%
    read_html() %>%
    html_nodes(".comment")
  
  vcard <- comment%>% html_nodes(".comment-author.vcard") %>% html_text() %>% str_remove_all("\t|\n") 
  comment_text <- comment%>% html_nodes(".comment-content") %>% html_text() %>% str_remove_all("\t|\n") 
  
  who <- str_match(vcard,"(.+)\\ssagte")[,2]
  datum <- str_match(vcard,"\\sam\\s(\\d+\\.\\s\\w+\\s\\d{4})\\sum")[,2]
  zeit <- str_match(vcard,"\\sum\\s(\\d+:\\d+)")[,2]

  datumzeit <- as.character(as.POSIXct(paste(datum,zeit),format = "%d. %B %Y %H:%M"))
  tibble(item_link = x, reply_author = who, reply_date = datumzeit, comment = comment_text)
}) %>%
  mutate(reply_date = as.POSIXct(reply_date))

write_csv(replies, "replies.csv")
```



### Twitter Data

```{r}
token <- create_token(
  app = "my_appname",
  consumer_key = "my_key",
  consumer_secret = "my_secret_key")
```

I can now get the tweets by \@raumzeit using the `get_timeline` function. 

```{r}
raumtweets <- get_timeline("raumzeit", n = 3200)
```

```{r}
raumtweets <- raumtweets %>%
  filter(!is_retweet) %>%
  dplyr::select(created_at,text,favorite_count,retweet_count,urls_expanded_url)
```


I particularly interested in the tweets regarding a specific podcast episode. Looking through the tweets I see that Tim usually uses the prefix "RZ" to refer to an episode, regrettably, he is not completely concise in this. Non the less, let's see how many episodes we can matchi in this way.

```{r}
raumtweets$rz_folge <- as.integer(str_match(raumtweets$text,"RZ(\\d{1,3})")[,2])
```

We've got 133 assigned tweets and 445 NAs with up to 5 tweets for a single episode. However, there is another way to assign a tweet to an episode: Via the URL linking to the episode. So let's extract these URLs from the column `urls_expanded_url`.

```{r, echo = TRUE}
raumtweets$item_link <- map_chr(raumtweets$urls_expanded_url,function(x){
  out <- x[str_detect(x,"raumzeit-podcast.de/\\d{4}")]
  ifelse(length(out)<0,NA_character_,out)
  })

```

We can see how many tweets we were able to assign with the two methods using `table()`
This shows that the first method was more successful than the second method.

```{r, echo = TRUE}
table(!is.na(raumtweets$rz_folge),!is.na(raumtweets$item_link))
```


```{r}
raumtweets_folgen <- raumtweets %>%
  left_join(select(filter(folgenuebersicht,!is.na(rz_folge)),title_clean,date,item_link), by = "item_link") %>%
  left_join(select(filter(folgenuebersicht,!is.na(rz_folge)),title_clean,date,rz_folge), by = "rz_folge") %>%
  mutate(
    title_clean = coalesce(title_clean.x,title_clean.y),
    date = coalesce(date.x,date.y)
  ) %>%
  select(-c(title_clean.x,title_clean.y,date.x,date.y,urls_expanded_url)) %>%
  filter(!is.na(title_clean))

write_csv(raumtweets_folgen,"raumtweets_folgen.csv")
```




